{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué queremos lograr?\n",
    "Crear un sistema de búsqueda de similitud de texto. El sistema va a recibir un texto y va a devolver los textos más similares a ese texto.\n",
    "\n",
    "## ¿Cómo lo vamos a construir?\n",
    "Así va la lógica:\n",
    "- Para almacenar el texto y poder encontrar los textos similares, usaremos una base de datos vectorial, para este proyecto se usará Chroma (Open Source y se puede instalar en local).\n",
    "- La base de datos vectorial almacena los textos como embeddings(representación de texto como vectores aka arrays de números). La transformación de texto a embeddings se hace con un modelo de lenguaje, en este caso usaremos un modelo de lenguaje de Hugging Face.\n",
    "- La clase que transforma texto a embeddings usa un tipo de dato llamado Document, que es un objeto que contiene el texto y metadata del texto.\n",
    "\n",
    "El flujo de trabajo será Documentos -> Embeddings -> Base de datos vectorial -> Búsqueda de similitud\n",
    "\n",
    "## Costos para este proyecto\n",
    "Ninguno, usaremos librerías y ejecutaremos todo en local.\n",
    "\n",
    "Con esto en mente, a codear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instalar dependencias** \n",
    "\n",
    "Psdt: El proyecto usa Poetry como manejador de dependencias.\n",
    "\n",
    "```bash\n",
    "poetry add langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto que usaremos para este proyecto es este blog de Hugging Face sobre embeddings: https://huggingface.co/blog/getting-started-with-embeddings.\n",
    "De su repo de Github descargué el archivo markdown y lo guardé en la carpeta `data` de este proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# Para usar UnstructuredMarkdownLoader, se debe instalar las librerías unstructured y markdown -> poetry add unstructured markdown\n",
    "\n",
    "proyect_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "file_path = f\"{proyect_root}/data/getting-started-with-embeddings.md\"\n",
    "loader = UnstructuredMarkdownLoader(file_path)\n",
    "document = loader.load() # devuelve una lista de Documents, en este caso solo hay uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"title: 'Getting Started With Embeddings'\\nthumbnail\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].page_content[:50] # primeros 1000 caracteres del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11997"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamaño del documento\n",
    "len(document[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Documento tiene 11997 caracteres, para que la base de datos vectorial devuelva el fragmento de texto más similar a la búsqueda se trabaja con fragmentos de texto más pequeños.\n",
    "\n",
    "Langchain tiene un separador de texto llamado RecursiveCharacterTextSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 250, # tamaño del chunk, tomar en cuenta el número de tokens que el modelo de lengua puede manejar\n",
    "    chunk_overlap  = 25, # entre 10% y 20% del tamaño del chunk\n",
    "    length_function = len # función que se aplica a cada chunk para obtener su longitud\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento se dividió en 70 chunks\n",
      "El primer chunk tiene 229 caracteres\n"
     ]
    }
   ],
   "source": [
    "print(f\"El documento se dividió en {len(chunks)} chunks\")\n",
    "print(f\"El primer chunk tiene {len(chunks[0].page_content)} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar con los embeddings, instalar la librería Sentence Transformers:\n",
    "\n",
    "```bash\n",
    "poetry add sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11249431d14d4fab8c298a5a4cfff664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f76c22b71bb4ee5b8ff686639bcb43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd6315d251b4e9d9b5ea0fbb0e7b186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b937d085fdc94de49b75af2d16e2c6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203e50b059104521acffbccbb3c6f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89fe358f846467c9989c6b4cd15fff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717451dc78b44ee2890e7bd68a946021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1c986b5c5e42a3affc9df9587f2cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6931dd583d412a88f3158bb26b22d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b9d2fdece2411a9fdd2bcdb29a3760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f681fbe0b4a45fe947b19fd1bd1192e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333aeb7777924b60977a10c669cd27e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b6c936af154ba9afd0a037b5810742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d100129fdc48e08864ad16b4ec43ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usaremos un modelo de embeddings de HuggingFace, aquí encuentras todos los disponibles: https://python.langchain.com/docs/integrations/text_embedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # más modelos en: https://huggingface.co/sentence-transformers\n",
    "model = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # información del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos el modelo tiene un máximo de 256 caracteres, así que la longitud de los Documentos debe ser menor a ese número.\n",
    "Y en Polling vemos la dimensionalidad de los embeddings, en este caso 384."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El embedding tiene 384 dimensiones\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de embedding\n",
    "embedding_example = model.embed_documents([chunks[0].page_content])[0]\n",
    "\n",
    "print(f\"El embedding tiene {len(embedding_example)} dimensiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con poetry instalar chromadb -> poetry add chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def get_chroma_db(documentos, modelo_embedding, ruta, recrear):\n",
    "  \"\"\"\n",
    "  Devuelve un objeto Chroma que se puede utilizar para la búsqueda de similitud.\n",
    "\n",
    "  Args:\n",
    "    documentos (list): Una lista de documentos para usar en la creación del objeto Chroma.\n",
    "    modelo_embedding (function): Una función que toma un documento como entrada y devuelve su embedding.\n",
    "    ruta (str): La ruta al directorio donde se debe persistir el objeto Chroma.\n",
    "    recrear (bool): Si es True, recrea el objeto Chroma desde cero. Si es False, carga el objeto Chroma existente desde el disco.\n",
    "\n",
    "  Returns:\n",
    "    Un objeto Chroma que se puede utilizar para la búsqueda de similitud.\n",
    "  \"\"\"\n",
    "  if recrear:\n",
    "    # Si recrear es True, crea un nuevo objeto Chroma desde cero\n",
    "    chroma = Chroma.from_documents(\n",
    "      documents=documentos,\n",
    "      embedding=modelo_embedding,\n",
    "      persist_directory=ruta,\n",
    "    )\n",
    "\n",
    "    chroma.persist()\n",
    "    return chroma\n",
    "  else:\n",
    "    # Si recrear es False, carga el objeto Chroma existente desde el disco\n",
    "    chroma = Chroma(\n",
    "      persist_directory=ruta,\n",
    "      embedding_function=modelo_embedding,\n",
    "    )\n",
    "    return chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_INDEX_NAME = \"similarity-search-101-chroma\"\n",
    "\n",
    "vector_store = get_chroma_db(\n",
    "    documentos=chunks,\n",
    "    modelo_embedding=model,\n",
    "    ruta=f\"{proyect_root}/db/{CHROMA_INDEX_NAME}\",\n",
    "    recrear=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a embedding model?\"\n",
    "\n",
    "docs = vector_store.similarity_search_with_score(query, k=5) # k es el número de documentos más similares que se devuelven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se devolvieron 5 documentos\n",
      "El primer documento devuelto tiene un score de 0.424575540971641 y su contenido es:\n",
      "An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc. The representation captures the semantic meaning of what is being embedded, making it robust for many industry applications.\n"
     ]
    }
   ],
   "source": [
    "# número de documentos devueltos\n",
    "print(f\"Se devolvieron {len(docs)} documentos\")\n",
    "# El primer documento devuelto: score y contenido\n",
    "print(f\"El primer documento devuelto tiene un score de {docs[0][1]} y su contenido es:\")\n",
    "print(docs[0][0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y viendo el contenido de la respuesta, sí es un texto que ayudaría a un modelo de lenguaje a como gpt-3 a responder preguntas la pregunta \"What is a embedding model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://python.langchain.com/docs/modules/data_connection/document_loaders/markdown\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub\n",
    "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-search-101-r2xOf8wD-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
