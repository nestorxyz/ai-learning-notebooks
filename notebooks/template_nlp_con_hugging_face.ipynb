{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgUNO42S9x3i"
      },
      "source": [
        "# NLP con Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPUO9PylwUcr"
      },
      "source": [
        "## Procesando los datos para NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxAWeUN9zsaT"
      },
      "source": [
        "### Descargando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b_HXFmANdVE1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('glue', 'mrpc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrDuinME8a61"
      },
      "source": [
        "Usaremos el dataset MRPC. Este es uno de los 10 datasets que componen el [benchmark (punto de referencia) GLUE](https://huggingface.co/datasets/glue). Se utiliza para medir el rendimiento de los modelos ML en 10 tareas de clasificación de texto diferentes.\n",
        "\n",
        "En otras palabras, seleccionamos el subset `mrpc` del dataset `glue`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sGMyV3iIdTUh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n"
          ]
        }
      ],
      "source": [
        "ex = ds['train'][0]\n",
        "print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "    num_rows: 3668\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# type of ex is dict\n",
        "print(ds['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRhoTyvBb_c"
      },
      "source": [
        "Así se ve un ejemplo. Notamos que `mrpc` está compuesto de dos oraciones y una etiqueta que indica si los dos enunciados son equivalentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A7xTuFa4zegd"
      },
      "outputs": [],
      "source": [
        "labels = ds['train'].features['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t_QPFmoZBmdR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'not_equivalent'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.int2str(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_At65Vv0Hmz"
      },
      "source": [
        "### Tokenizando\n",
        "\n",
        "¿Recuerdas que con visión descargamos el feature extractor directamente del repositorio del modelo pre-entrenado que vamos a usar como base?\n",
        "\n",
        "Podemos pensar en la función tokenizadora como el equivalente en el NLP.\n",
        "\n",
        "Descargamos el tokenizador directamente del repo del modelo que usaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j02Vp1mo0hVq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "repo_id = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUqXIm4fDi3W"
      },
      "source": [
        "Para preprocesar el conjunto de datos necesitamos convertir el texto en números que el modelo pueda entender. Esto se hace con un tokenizador. \n",
        "\n",
        "Pasar de texto a números se conoce como codificación o encoding. El encoding se realiza en un proceso de dos pasos: la tokenización, seguida de la conversión a input ids. Por el momento nos basta saber que estamos traduciendo texto a números llamados como input ids. Estos estarán en el formato adecuado para alimentar nuestro modelo.\n",
        "\n",
        "Podemos alimentar al tokenizador con una oración o una lista de oraciones, por lo que podemos tokenizar directamente todas las primeras oraciones y todas las segundas oraciones de cada par de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "huibMGKOD-Ut"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentece_1 = tokenizer(ds['train']['sentence1'][2])\n",
        "tokenized_sentece_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBrkzA08EslP"
      },
      "source": [
        "Necesitamos manejar los dos enunciados como un par y no separados. El tokenizador puede tomar un par de secuencias y prepararlas de la manera que espera nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3O7kBG_bE18h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzgBI7nGC31"
      },
      "source": [
        "¿Qué significa cada uno de los valores que nos retorna el tokenizador?\n",
        "- `input_ids` es la traducción de palabras a números.\n",
        "- `attention_mask` es un tensor con la misma forma que `input_ids`, pero lleno de 0 y 1: los 1 indican que se debe atender a los tokens correspondientes y los 0 indican que no se deben atender. Es decir, deben ser ignorados por el modelo.\n",
        "- `token_type_ids` dice al modelo qué parte de la entrada es la primera oración y cuál es la segunda oración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ewj-TxGj4P"
      },
      "source": [
        "El modelo espera que las entradas sean de la forma [CLS] oración 1 [SEP] oración 2 [SEP] cuando hay dos oraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w6IL1vRNjehn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'one',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOzRDDXMG6KS"
      },
      "source": [
        "Si seleccionamos otro modelo en el Hub no necesariamente tendremos `token_type_ids` en las entradas tokenizadas (por ejemplo, no se devuelven si usa un modelo `DistilBERT`). Solo se devuelven cuando el modelo sabrá qué hacer con ellas, porque los ha visto durante su preentrenamiento.\n",
        "\n",
        "En general, no necesitamos preocuparnos por si hay o no `token_type_ids` en nuestras entradas tokenizadas, siempre que usemos el tokenizador correspondiente al modelo, todo estará bien ya que el tokenizador sabe qué proporcionar al modelo.\n",
        "\n",
        "Por ejemplo, durante esta clase utilizaremos un modelo [`distilroberta-base`](https://huggingface.co/distilroberta-base) por su tamaño y efectividad. Pero no cuenta con `token_type_ids` y aún así nos regresa excelentes resultados.\n",
        "\n",
        "En la organización del Platzi en el Hub puedes encontrar un [modelo BERT](https://huggingface.co/platzi/platzi-distilroberta-base-mrpc-glue-omar-espejel) afinado siguiendo el mismo proceso que usamos en esta clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kUn74FigzhHm"
      },
      "outputs": [],
      "source": [
        "repo_id = \"distilroberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlcHOXcQ2-kW"
      },
      "source": [
        "Creamos una función tokenizadora. Recibe un ejemplo y lo tokeniza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1hkNcAjX0nSJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "49j2GVph0swg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea85b143be6400c8753010fe4898220",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "154fb2e74fec484f97d6ba3d6d2a7eda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ba0d9b7e95144d887628da77f99ea51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prepared_ds = ds.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIsjhHyf3O_L"
      },
      "source": [
        "### Definiendo el data collator: Dynamic padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIH2CWjNyQ13"
      },
      "source": [
        "Necesitamos que nuestros tensores tengan una forma rectangular. Es decir que tengan el mismo tamaño cada uno de los ejemplos. Sin embargo, los textos no necesariamente tienen el mismo tamaño. \n",
        "\n",
        "Para ello usamos el relleno o padding. El padding se asegura de que todas nuestras oraciones tengan la misma longitud al agregar una palabra especial llamada padding token a las oraciones con menos valores. Por ejemplo, si tenemos 10 oraciones con 10 palabras y 1 oración con 20 palabras, el relleno garantizará que todas las oraciones tengan 20 palabras.\n",
        "\n",
        "Dejamos el argumento de `padding` del tokenizer vacío en nuestra función de tokenización por ahora. Esto se debe a que rellenar (hacer padding) todas las muestras hasta la longitud máxima del dataset no es eficiente, es mejor rellenar las muestras cuando estamos construyendo un batch, ya que entonces solo necesitamos rellenar hasta la longitud máxima en ese batch, y no la longitud máxima en todo el dataset. ¡Esto puede ahorrar mucho tiempo y potencia de procesamiento cuando las entradas tienen longitudes muy variables!\n",
        "\n",
        "Usaremos un DataCollator para esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHKCmEdneI7X"
      },
      "source": [
        "Rellenemos (hagamos padding) todos los ejemplos con la longitud del elemento más largo del batch. A esta técnica se le conoce como relleno dinámico o dynamic padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JMjUzWDK3SzH"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpONY8Hl3Yfb"
      },
      "source": [
        "## Entrenamiento y evaluación\n",
        "\n",
        "Definamos el resto de los argumentos necesarios para `Trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfIjQnh3bhn"
      },
      "source": [
        "### Definiendo la métrica "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z_h5Pm9e3ZGi"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erd8wVZ13l-8"
      },
      "source": [
        "### Configurando `Trainer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dy9gQdTj3q1q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "labels = ds['train'].features['label'].names\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "  repo_id, \n",
        "  num_labels=len(labels), \n",
        "  id2label={str(i): label for i, label in enumerate(labels)}, \n",
        "  label2id={label: i for i, label in enumerate(labels)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DW93t2gG4luP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nestorxyz/Library/Caches/pypoetry/virtualenvs/similarity-search-101-r2xOf8wD-py3.9/lib/python3.9/site-packages/transformers/training_args.py:1738: FutureWarning: `--push_to_hub_organization` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case platzi/transfer-course-distilroberta-base-mrpc-glue-nestor-mamani).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='./transfer-course-distilroberta-base-mrpc-glue-nestor-mamani',          # output directory\n",
        "  num_train_epochs=3,              # total number of training epochs\n",
        "  per_device_train_batch_size=16,  # batch size per device during training\n",
        "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "  weight_decay=0.01,               # strength of weight decay\n",
        "  logging_dir='./logs',            # directory for storing logs\n",
        "  logging_steps=10,\n",
        "  evaluation_strategy='steps',\n",
        "  eval_steps=500,\n",
        "  load_best_model_at_end=True,\n",
        "  push_to_hub=True,\n",
        "  push_to_hub_organization=\"platzi\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GkTQ7pL85UI3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec52ba02b1c74329a446da31a05acca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4ek7pNKuyNJn"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "  args=training_args,                  # training arguments, defined above\n",
        "  train_dataset=prepared_ds['train'],         # training dataset\n",
        "  eval_dataset=prepared_ds['validation'],             # evaluation dataset\n",
        "  data_collator=data_collator,\n",
        "  tokenizer=tokenizer,\n",
        "  compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTZJl_b_6AvW"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fyrp6fmGy3e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97d418ad63d848c09432c774e8f1132b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/690 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7001, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
            "{'loss': 0.6929, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.09}\n",
            "{'loss': 0.6827, 'learning_rate': 3e-06, 'epoch': 0.13}\n",
            "{'loss': 0.6752, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.17}\n",
            "{'loss': 0.657, 'learning_rate': 5e-06, 'epoch': 0.22}\n",
            "{'loss': 0.6256, 'learning_rate': 6e-06, 'epoch': 0.26}\n",
            "{'loss': 0.6543, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.3}\n",
            "{'loss': 0.6606, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.35}\n",
            "{'loss': 0.6093, 'learning_rate': 9e-06, 'epoch': 0.39}\n",
            "{'loss': 0.5816, 'learning_rate': 1e-05, 'epoch': 0.43}\n",
            "{'loss': 0.6384, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.48}\n",
            "{'loss': 0.5593, 'learning_rate': 1.2e-05, 'epoch': 0.52}\n",
            "{'loss': 0.6282, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.57}\n",
            "{'loss': 0.6028, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.61}\n",
            "{'loss': 0.5705, 'learning_rate': 1.5e-05, 'epoch': 0.65}\n",
            "{'loss': 0.489, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.7}\n",
            "{'loss': 0.5814, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.74}\n",
            "{'loss': 0.591, 'learning_rate': 1.8e-05, 'epoch': 0.78}\n",
            "{'loss': 0.4698, 'learning_rate': 1.9e-05, 'epoch': 0.83}\n",
            "{'loss': 0.5443, 'learning_rate': 2e-05, 'epoch': 0.87}\n",
            "{'loss': 0.479, 'learning_rate': 2.1e-05, 'epoch': 0.91}\n",
            "{'loss': 0.444, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.96}\n",
            "{'loss': 0.5519, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4841, 'learning_rate': 2.4e-05, 'epoch': 1.04}\n",
            "{'loss': 0.4872, 'learning_rate': 2.5e-05, 'epoch': 1.09}\n",
            "{'loss': 0.4723, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.13}\n",
            "{'loss': 0.3867, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.17}\n",
            "{'loss': 0.4755, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3985, 'learning_rate': 2.9e-05, 'epoch': 1.26}\n",
            "{'loss': 0.3545, 'learning_rate': 3e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3693, 'learning_rate': 3.1e-05, 'epoch': 1.35}\n",
            "{'loss': 0.4665, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.39}\n",
            "{'loss': 0.449, 'learning_rate': 3.3e-05, 'epoch': 1.43}\n",
            "{'loss': 0.3555, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.48}\n",
            "{'loss': 0.3023, 'learning_rate': 3.5e-05, 'epoch': 1.52}\n",
            "{'loss': 0.4647, 'learning_rate': 3.6e-05, 'epoch': 1.57}\n",
            "{'loss': 0.3737, 'learning_rate': 3.7e-05, 'epoch': 1.61}\n",
            "{'loss': 0.4793, 'learning_rate': 3.8e-05, 'epoch': 1.65}\n",
            "{'loss': 0.4287, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.7}\n",
            "{'loss': 0.4932, 'learning_rate': 4e-05, 'epoch': 1.74}\n",
            "{'loss': 0.4478, 'learning_rate': 4.1e-05, 'epoch': 1.78}\n",
            "{'loss': 0.4218, 'learning_rate': 4.2e-05, 'epoch': 1.83}\n",
            "{'loss': 0.4692, 'learning_rate': 4.3e-05, 'epoch': 1.87}\n",
            "{'loss': 0.4625, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.91}\n",
            "{'loss': 0.3859, 'learning_rate': 4.5e-05, 'epoch': 1.96}\n",
            "{'loss': 0.4561, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3794, 'learning_rate': 4.7e-05, 'epoch': 2.04}\n",
            "{'loss': 0.3019, 'learning_rate': 4.8e-05, 'epoch': 2.09}\n",
            "{'loss': 0.3189, 'learning_rate': 4.9e-05, 'epoch': 2.13}\n",
            "{'loss': 0.315, 'learning_rate': 5e-05, 'epoch': 2.17}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7920093db724b10ac7f4e4484f2dd82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b33a69c9c88435c8dbc983b8758e7b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.46014174818992615, 'eval_accuracy': 0.8357843137254902, 'eval_f1': 0.8858603066439524, 'eval_runtime': 2.9507, 'eval_samples_per_second': 138.271, 'eval_steps_per_second': 2.372, 'epoch': 2.17}\n",
            "{'loss': 0.2844, 'learning_rate': 4.736842105263158e-05, 'epoch': 2.22}\n",
            "{'loss': 0.1503, 'learning_rate': 4.473684210526316e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3402, 'learning_rate': 4.210526315789474e-05, 'epoch': 2.3}\n",
            "{'loss': 0.279, 'learning_rate': 3.9473684210526316e-05, 'epoch': 2.35}\n",
            "{'loss': 0.3684, 'learning_rate': 3.6842105263157895e-05, 'epoch': 2.39}\n",
            "{'loss': 0.3793, 'learning_rate': 3.421052631578947e-05, 'epoch': 2.43}\n",
            "{'loss': 0.2879, 'learning_rate': 3.157894736842105e-05, 'epoch': 2.48}\n",
            "{'loss': 0.3529, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.52}\n",
            "{'loss': 0.2574, 'learning_rate': 2.6315789473684212e-05, 'epoch': 2.57}\n",
            "{'loss': 0.4201, 'learning_rate': 2.368421052631579e-05, 'epoch': 2.61}\n",
            "{'loss': 0.2821, 'learning_rate': 2.105263157894737e-05, 'epoch': 2.65}\n",
            "{'loss': 0.2328, 'learning_rate': 1.8421052631578947e-05, 'epoch': 2.7}\n",
            "{'loss': 0.2919, 'learning_rate': 1.5789473684210526e-05, 'epoch': 2.74}\n",
            "{'loss': 0.196, 'learning_rate': 1.3157894736842106e-05, 'epoch': 2.78}\n",
            "{'loss': 0.2064, 'learning_rate': 1.0526315789473684e-05, 'epoch': 2.83}\n",
            "{'loss': 0.2164, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.87}\n",
            "{'loss': 0.4079, 'learning_rate': 5.263157894736842e-06, 'epoch': 2.91}\n",
            "{'loss': 0.2029, 'learning_rate': 2.631578947368421e-06, 'epoch': 2.96}\n",
            "{'loss': 0.2685, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "{'train_runtime': 139.6582, 'train_samples_per_second': 78.792, 'train_steps_per_second': 4.941, 'train_loss': 0.43931484187858694, 'epoch': 3.0}\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.4393\n",
            "  train_runtime            = 0:02:19.65\n",
            "  train_samples_per_second =     78.792\n",
            "  train_steps_per_second   =      4.941\n"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ-2BxDN6TjY"
      },
      "source": [
        "### Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KX7GIg8b6Wsn"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e059613aab124dbcbf3fddf836e0c456",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8358\n",
            "  eval_f1                 =     0.8859\n",
            "  eval_loss               =     0.4601\n",
            "  eval_runtime            = 0:00:01.84\n",
            "  eval_samples_per_second =    220.794\n",
            "  eval_steps_per_second   =      3.788\n"
          ]
        }
      ],
      "source": [
        "metrics = trainer.evaluate(prepared_ds['validation'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPxunFyM6fLy"
      },
      "source": [
        "### Compartimos en el Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fowyC7_56W0S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
