{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgUNO42S9x3i"
      },
      "source": [
        "# NLP con Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPUO9PylwUcr"
      },
      "source": [
        "## Procesando los datos para NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxAWeUN9zsaT"
      },
      "source": [
        "### Descargando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b_HXFmANdVE1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('glue', 'mrpc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrDuinME8a61"
      },
      "source": [
        "Usaremos el dataset MRPC. Este es uno de los 10 datasets que componen el [benchmark (punto de referencia) GLUE](https://huggingface.co/datasets/glue). Se utiliza para medir el rendimiento de los modelos ML en 10 tareas de clasificaci√≥n de texto diferentes.\n",
        "\n",
        "En otras palabras, seleccionamos el subset `mrpc` del dataset `glue`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sGMyV3iIdTUh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n"
          ]
        }
      ],
      "source": [
        "ex = ds['train'][0]\n",
        "print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "    num_rows: 3668\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# type of ex is dict\n",
        "print(ds['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRhoTyvBb_c"
      },
      "source": [
        "As√≠ se ve un ejemplo. Notamos que `mrpc` est√° compuesto de dos oraciones y una etiqueta que indica si los dos enunciados son equivalentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A7xTuFa4zegd"
      },
      "outputs": [],
      "source": [
        "labels = ds['train'].features['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t_QPFmoZBmdR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'not_equivalent'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.int2str(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_At65Vv0Hmz"
      },
      "source": [
        "### Tokenizando\n",
        "\n",
        "¬øRecuerdas que con visi√≥n descargamos el feature extractor directamente del repositorio del modelo pre-entrenado que vamos a usar como base?\n",
        "\n",
        "Podemos pensar en la funci√≥n tokenizadora como el equivalente en el NLP.\n",
        "\n",
        "Descargamos el tokenizador directamente del repo del modelo que usaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j02Vp1mo0hVq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "repo_id = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUqXIm4fDi3W"
      },
      "source": [
        "Para preprocesar el conjunto de datos necesitamos convertir el texto en n√∫meros que el modelo pueda entender. Esto se hace con un tokenizador. \n",
        "\n",
        "Pasar de texto a n√∫meros se conoce como codificaci√≥n o encoding. El encoding se realiza en un proceso de dos pasos: la tokenizaci√≥n, seguida de la conversi√≥n a input ids. Por el momento nos basta saber que estamos traduciendo texto a n√∫meros llamados como input ids. Estos estar√°n en el formato adecuado para alimentar nuestro modelo.\n",
        "\n",
        "Podemos alimentar al tokenizador con una oraci√≥n o una lista de oraciones, por lo que podemos tokenizar directamente todas las primeras oraciones y todas las segundas oraciones de cada par de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "huibMGKOD-Ut"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentece_1 = tokenizer(ds['train']['sentence1'][2])\n",
        "tokenized_sentece_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBrkzA08EslP"
      },
      "source": [
        "Necesitamos manejar los dos enunciados como un par y no separados. El tokenizador puede tomar un par de secuencias y prepararlas de la manera que espera nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3O7kBG_bE18h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzgBI7nGC31"
      },
      "source": [
        "¬øQu√© significa cada uno de los valores que nos retorna el tokenizador?\n",
        "- `input_ids` es la traducci√≥n de palabras a n√∫meros.\n",
        "- `attention_mask` es un tensor con la misma forma que `input_ids`, pero lleno de 0 y 1: los 1 indican que se debe atender a los tokens correspondientes y los 0 indican que no se deben atender. Es decir, deben ser ignorados por el modelo.\n",
        "- `token_type_ids` dice al modelo qu√© parte de la entrada es la primera oraci√≥n y cu√°l es la segunda oraci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ewj-TxGj4P"
      },
      "source": [
        "El modelo espera que las entradas sean de la forma [CLS] oraci√≥n 1 [SEP] oraci√≥n 2 [SEP] cuando hay dos oraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w6IL1vRNjehn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'one',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOzRDDXMG6KS"
      },
      "source": [
        "Si seleccionamos otro modelo en el Hub no necesariamente tendremos `token_type_ids` en las entradas tokenizadas (por ejemplo, no se devuelven si usa un modelo `DistilBERT`). Solo se devuelven cuando el modelo sabr√° qu√© hacer con ellas, porque los ha visto durante su preentrenamiento.\n",
        "\n",
        "En general, no necesitamos preocuparnos por si hay o no `token_type_ids` en nuestras entradas tokenizadas, siempre que usemos el tokenizador correspondiente al modelo, todo estar√° bien ya que el tokenizador sabe qu√© proporcionar al modelo.\n",
        "\n",
        "Por ejemplo, durante esta clase utilizaremos un modelo [`distilroberta-base`](https://huggingface.co/distilroberta-base) por su tama√±o y efectividad. Pero no cuenta con `token_type_ids` y a√∫n as√≠ nos regresa excelentes resultados.\n",
        "\n",
        "En la organizaci√≥n del Platzi en el Hub puedes encontrar un [modelo BERT](https://huggingface.co/platzi/platzi-distilroberta-base-mrpc-glue-omar-espejel) afinado siguiendo el mismo proceso que usamos en esta clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kUn74FigzhHm"
      },
      "outputs": [],
      "source": [
        "repo_id = \"distilroberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlcHOXcQ2-kW"
      },
      "source": [
        "Creamos una funci√≥n tokenizadora. Recibe un ejemplo y lo tokeniza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1hkNcAjX0nSJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "49j2GVph0swg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea85b143be6400c8753010fe4898220",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "154fb2e74fec484f97d6ba3d6d2a7eda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ba0d9b7e95144d887628da77f99ea51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prepared_ds = ds.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIsjhHyf3O_L"
      },
      "source": [
        "### Definiendo el data collator: Dynamic padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIH2CWjNyQ13"
      },
      "source": [
        "Necesitamos que nuestros tensores tengan una forma rectangular. Es decir que tengan el mismo tama√±o cada uno de los ejemplos. Sin embargo, los textos no necesariamente tienen el mismo tama√±o. \n",
        "\n",
        "Para ello usamos el relleno o padding. El padding se asegura de que todas nuestras oraciones tengan la misma longitud al agregar una palabra especial llamada padding token a las oraciones con menos valores. Por ejemplo, si tenemos 10 oraciones con 10 palabras y 1 oraci√≥n con 20 palabras, el relleno garantizar√° que todas las oraciones tengan 20 palabras.\n",
        "\n",
        "Dejamos el argumento de `padding` del tokenizer vac√≠o en nuestra funci√≥n de tokenizaci√≥n por ahora. Esto se debe a que rellenar (hacer padding) todas las muestras hasta la longitud m√°xima del dataset no es eficiente, es mejor rellenar las muestras cuando estamos construyendo un batch, ya que entonces solo necesitamos rellenar hasta la longitud m√°xima en ese batch, y no la longitud m√°xima en todo el dataset. ¬°Esto puede ahorrar mucho tiempo y potencia de procesamiento cuando las entradas tienen longitudes muy variables!\n",
        "\n",
        "Usaremos un DataCollator para esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHKCmEdneI7X"
      },
      "source": [
        "Rellenemos (hagamos padding) todos los ejemplos con la longitud del elemento m√°s largo del batch. A esta t√©cnica se le conoce como relleno din√°mico o dynamic padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JMjUzWDK3SzH"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpONY8Hl3Yfb"
      },
      "source": [
        "## Entrenamiento y evaluaci√≥n\n",
        "\n",
        "Definamos el resto de los argumentos necesarios para `Trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfIjQnh3bhn"
      },
      "source": [
        "### Definiendo la m√©trica "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z_h5Pm9e3ZGi"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erd8wVZ13l-8"
      },
      "source": [
        "### Configurando `Trainer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dy9gQdTj3q1q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "labels = ds['train'].features['label'].names\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "  repo_id, \n",
        "  num_labels=len(labels), \n",
        "  id2label={str(i): label for i, label in enumerate(labels)}, \n",
        "  label2id={label: i for i, label in enumerate(labels)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DW93t2gG4luP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nestorxyz/Library/Caches/pypoetry/virtualenvs/similarity-search-101-r2xOf8wD-py3.9/lib/python3.9/site-packages/transformers/training_args.py:1738: FutureWarning: `--push_to_hub_organization` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case platzi/transfer-course-distilroberta-base-mrpc-glue-nestor-mamani).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='./transfer-course-distilroberta-base-mrpc-glue-nestor-mamani',          # output directory\n",
        "  num_train_epochs=3,              # total number of training epochs\n",
        "  per_device_train_batch_size=16,  # batch size per device during training\n",
        "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "  weight_decay=0.01,               # strength of weight decay\n",
        "  logging_dir='./logs',            # directory for storing logs\n",
        "  logging_steps=10,\n",
        "  evaluation_strategy='steps',\n",
        "  eval_steps=500,\n",
        "  load_best_model_at_end=True,\n",
        "  push_to_hub=True,\n",
        "  push_to_hub_organization=\"platzi\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GkTQ7pL85UI3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec52ba02b1c74329a446da31a05acca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4ek7pNKuyNJn"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "  args=training_args,                  # training arguments, defined above\n",
        "  train_dataset=prepared_ds['train'],         # training dataset\n",
        "  eval_dataset=prepared_ds['validation'],             # evaluation dataset\n",
        "  data_collator=data_collator,\n",
        "  tokenizer=tokenizer,\n",
        "  compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTZJl_b_6AvW"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fyrp6fmGy3e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97d418ad63d848c09432c774e8f1132b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/690 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7001, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
            "{'loss': 0.6929, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.09}\n",
            "{'loss': 0.6827, 'learning_rate': 3e-06, 'epoch': 0.13}\n",
            "{'loss': 0.6752, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.17}\n",
            "{'loss': 0.657, 'learning_rate': 5e-06, 'epoch': 0.22}\n",
            "{'loss': 0.6256, 'learning_rate': 6e-06, 'epoch': 0.26}\n",
            "{'loss': 0.6543, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.3}\n",
            "{'loss': 0.6606, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.35}\n",
            "{'loss': 0.6093, 'learning_rate': 9e-06, 'epoch': 0.39}\n",
            "{'loss': 0.5816, 'learning_rate': 1e-05, 'epoch': 0.43}\n",
            "{'loss': 0.6384, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.48}\n",
            "{'loss': 0.5593, 'learning_rate': 1.2e-05, 'epoch': 0.52}\n",
            "{'loss': 0.6282, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.57}\n",
            "{'loss': 0.6028, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.61}\n",
            "{'loss': 0.5705, 'learning_rate': 1.5e-05, 'epoch': 0.65}\n",
            "{'loss': 0.489, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.7}\n",
            "{'loss': 0.5814, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.74}\n",
            "{'loss': 0.591, 'learning_rate': 1.8e-05, 'epoch': 0.78}\n",
            "{'loss': 0.4698, 'learning_rate': 1.9e-05, 'epoch': 0.83}\n",
            "{'loss': 0.5443, 'learning_rate': 2e-05, 'epoch': 0.87}\n",
            "{'loss': 0.479, 'learning_rate': 2.1e-05, 'epoch': 0.91}\n",
            "{'loss': 0.444, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.96}\n",
            "{'loss': 0.5519, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4841, 'learning_rate': 2.4e-05, 'epoch': 1.04}\n",
            "{'loss': 0.4872, 'learning_rate': 2.5e-05, 'epoch': 1.09}\n",
            "{'loss': 0.4723, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.13}\n",
            "{'loss': 0.3867, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.17}\n",
            "{'loss': 0.4755, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3985, 'learning_rate': 2.9e-05, 'epoch': 1.26}\n",
            "{'loss': 0.3545, 'learning_rate': 3e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3693, 'learning_rate': 3.1e-05, 'epoch': 1.35}\n",
            "{'loss': 0.4665, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.39}\n",
            "{'loss': 0.449, 'learning_rate': 3.3e-05, 'epoch': 1.43}\n",
            "{'loss': 0.3555, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.48}\n",
            "{'loss': 0.3023, 'learning_rate': 3.5e-05, 'epoch': 1.52}\n",
            "{'loss': 0.4647, 'learning_rate': 3.6e-05, 'epoch': 1.57}\n",
            "{'loss': 0.3737, 'learning_rate': 3.7e-05, 'epoch': 1.61}\n",
            "{'loss': 0.4793, 'learning_rate': 3.8e-05, 'epoch': 1.65}\n",
            "{'loss': 0.4287, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.7}\n",
            "{'loss': 0.4932, 'learning_rate': 4e-05, 'epoch': 1.74}\n",
            "{'loss': 0.4478, 'learning_rate': 4.1e-05, 'epoch': 1.78}\n",
            "{'loss': 0.4218, 'learning_rate': 4.2e-05, 'epoch': 1.83}\n",
            "{'loss': 0.4692, 'learning_rate': 4.3e-05, 'epoch': 1.87}\n",
            "{'loss': 0.4625, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.91}\n",
            "{'loss': 0.3859, 'learning_rate': 4.5e-05, 'epoch': 1.96}\n",
            "{'loss': 0.4561, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3794, 'learning_rate': 4.7e-05, 'epoch': 2.04}\n",
            "{'loss': 0.3019, 'learning_rate': 4.8e-05, 'epoch': 2.09}\n",
            "{'loss': 0.3189, 'learning_rate': 4.9e-05, 'epoch': 2.13}\n",
            "{'loss': 0.315, 'learning_rate': 5e-05, 'epoch': 2.17}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7920093db724b10ac7f4e4484f2dd82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b33a69c9c88435c8dbc983b8758e7b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.46014174818992615, 'eval_accuracy': 0.8357843137254902, 'eval_f1': 0.8858603066439524, 'eval_runtime': 2.9507, 'eval_samples_per_second': 138.271, 'eval_steps_per_second': 2.372, 'epoch': 2.17}\n",
            "{'loss': 0.2844, 'learning_rate': 4.736842105263158e-05, 'epoch': 2.22}\n",
            "{'loss': 0.1503, 'learning_rate': 4.473684210526316e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3402, 'learning_rate': 4.210526315789474e-05, 'epoch': 2.3}\n",
            "{'loss': 0.279, 'learning_rate': 3.9473684210526316e-05, 'epoch': 2.35}\n",
            "{'loss': 0.3684, 'learning_rate': 3.6842105263157895e-05, 'epoch': 2.39}\n",
            "{'loss': 0.3793, 'learning_rate': 3.421052631578947e-05, 'epoch': 2.43}\n",
            "{'loss': 0.2879, 'learning_rate': 3.157894736842105e-05, 'epoch': 2.48}\n",
            "{'loss': 0.3529, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.52}\n",
            "{'loss': 0.2574, 'learning_rate': 2.6315789473684212e-05, 'epoch': 2.57}\n",
            "{'loss': 0.4201, 'learning_rate': 2.368421052631579e-05, 'epoch': 2.61}\n",
            "{'loss': 0.2821, 'learning_rate': 2.105263157894737e-05, 'epoch': 2.65}\n",
            "{'loss': 0.2328, 'learning_rate': 1.8421052631578947e-05, 'epoch': 2.7}\n",
            "{'loss': 0.2919, 'learning_rate': 1.5789473684210526e-05, 'epoch': 2.74}\n",
            "{'loss': 0.196, 'learning_rate': 1.3157894736842106e-05, 'epoch': 2.78}\n",
            "{'loss': 0.2064, 'learning_rate': 1.0526315789473684e-05, 'epoch': 2.83}\n",
            "{'loss': 0.2164, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.87}\n",
            "{'loss': 0.4079, 'learning_rate': 5.263157894736842e-06, 'epoch': 2.91}\n",
            "{'loss': 0.2029, 'learning_rate': 2.631578947368421e-06, 'epoch': 2.96}\n",
            "{'loss': 0.2685, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "{'train_runtime': 139.6582, 'train_samples_per_second': 78.792, 'train_steps_per_second': 4.941, 'train_loss': 0.43931484187858694, 'epoch': 3.0}\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.4393\n",
            "  train_runtime            = 0:02:19.65\n",
            "  train_samples_per_second =     78.792\n",
            "  train_steps_per_second   =      4.941\n"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ-2BxDN6TjY"
      },
      "source": [
        "### Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KX7GIg8b6Wsn"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e059613aab124dbcbf3fddf836e0c456",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8358\n",
            "  eval_f1                 =     0.8859\n",
            "  eval_loss               =     0.4601\n",
            "  eval_runtime            = 0:00:01.84\n",
            "  eval_samples_per_second =    220.794\n",
            "  eval_steps_per_second   =      3.788\n"
          ]
        }
      ],
      "source": [
        "metrics = trainer.evaluate(prepared_ds['validation'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPxunFyM6fLy"
      },
      "source": [
        "### Compartimos en el Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fowyC7_56W0S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
